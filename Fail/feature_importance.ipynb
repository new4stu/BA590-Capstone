{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import copy\n",
    "\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'read_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-869001337d12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Full_Datav5.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc_cat_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'read_data' is not defined"
     ]
    }
   ],
   "source": [
    "df, y = read_data.csv('../Full_Datav5.csv')\n",
    "df = proc_cat_df(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will build a baseline model, in which we will oversample the target feature to ensure balance in its classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X = df.drop('Y', axis = 1)\n",
    "y = df['Y']\n",
    "X.head()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=2019)\n",
    "\n",
    "#begin oversampling\n",
    "oversample = pd.concat([X_train,y_train],axis=1)\n",
    "max_size = oversample['Y'].value_counts().max()\n",
    "lst = [oversample]\n",
    "\n",
    "for class_index, group in oversample.groupby('Y'):\n",
    "        lst.append(group.sample(max_size-len(group), replace=True))\n",
    "X_train = pd.concat(lst)\n",
    "y_train=pd.DataFrame.copy(X_train['Y'])\n",
    "del X_train['Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensure that above worked\n",
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#establish baseline model to tweak\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "clf = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_val)\n",
    "\n",
    "print(f\"single run validation accuracy score: {accuracy_score(y_val, y_pred)}\")\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to try and make a better model, we will try a random search of hyperparameters of a Random Forest to see what values our random search yields. From there, we can then use a narrower Grid search to find our best performing model, which will be our model to help find the best features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try changing the number of trees\n",
    "\n",
    "clf1 = RandomForestClassifier(n_estimators = 20,n_jobs=-1, oob_score=True)\n",
    "\n",
    "clf1.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred1 = clf1.predict(X_train)\n",
    "y_val_pred1 = clf1.predict(X_val)\n",
    "\n",
    "print(f\"single run training accuracy: {accuracy_score(y_train, y_train_pred1)}\")\n",
    "print(f\"single run validation accuracy: {accuracy_score(y_val, y_val_pred1)}\")\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This slightly boosted our score, so let's try more trees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try changing the number of trees\n",
    "\n",
    "clf2 = RandomForestClassifier(n_estimators = 80,n_jobs=-1, oob_score=True)\n",
    "\n",
    "clf2.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred2 = clf2.predict(X_train)\n",
    "y_val_pred2 = clf2.predict(X_val)\n",
    "\n",
    "print(f\"single run training accuracy: {accuracy_score(y_train, y_train_pred2)}\")\n",
    "print(f\"single run validation accuracy: {accuracy_score(y_val, y_val_pred2)}\")\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This did not help again, so now we will try a Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#define number of trees \n",
    "n_estimators = [10, 20, 40, 80]\n",
    "\n",
    "#define min number of samples at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'min_samples_leaf': min_samples_leaf\n",
    "              }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform the grid seach, using 3-fold cross_validation\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "rf_grid = GridSearchCV(estimator=clf, param_grid=random_grid,\n",
    "                         scoring = 'precision',\n",
    "                         cv = 3,\n",
    "                         n_jobs = -1\n",
    "                              )\n",
    "\n",
    "#fit the model\n",
    "%timeit rf_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use these parameters for final model\n",
    "\n",
    "clf_final = RandomForestClassifier(min_samples_leaf= 1, n_estimators= 40,\n",
    "                                   n_jobs = -1\n",
    "                                  )\n",
    "\n",
    "clf_final.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_final = clf_final.predict(X_val)\n",
    "\n",
    "print(classification_report(y_val, y_pred_final))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance\n",
    "\n",
    "For this next part, we are going to compare different ways of feature importance measures. Specifically, we'll look at what sklearn calculates, and then we will use two independent techniques: ELI5 library, and Shapley values. Both will be briefly explaned in theory and will be utilized via independent 3rd party libraries. \n",
    "\n",
    "To start, we will use sklearn's built in feature importance measures with our fully-built Random Forest model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "feature_imp = pd.Series(clf_final.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
    "feature_imp.plot(kind = 'barh')\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.xlabel('Importance score')\n",
    "plt.ylabel('Feature')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this chart, we can see that the most important feature in the model is 'PAY_1', the amount one pays in the month before the current month. Then, there are many features that have a similar importance to each other, with a drop off in importance starting with 'PAY_6'. From a wholistic perspective, it seems the only really important feature about a customer is his or her age. The more important features are more so about the bill amounts and the payment amounts. It's all about the money!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HOWEVER, SCIKIT LEARN DOES NOT CALCULATE THESE VALEUS WELL!!!\n",
    "\n",
    "\n",
    "This, of course, is useful, but one question that the modeler needs to always ask themselves is how much can he or she trust these findings? First off, do these findings make sense? Is age really the only important attributea about a customer? Second of all, we don't know for sure how the values of the features impact the model's thinking. For instance, could it be that the model acutally found that younger customers had a _lower_ chance of defaulting, when in the real world it could be the complete opposite? If this were the case, one would not have complete trust that this model is learning the appropriate patterns in the data; it might be performing well due to false findings.\n",
    "\n",
    "In order to ensure our models findings, it would be helpful if we could see how the values of the features impact the model's predictions. From this idea we introduce ELI5, an independent library that shows ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val.values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = df.columns.drop('Y')\n",
    "column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "#rf = clf_final.fit(X_train, y_train)\n",
    "#perm = PermutationImportance(rf).fit(X_val, y_val)\n",
    "eli5.show_weights(perm, feature_names = list(column), top=27)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, we can see that the most important features and their order are in fact different from sklearn's ordering. Why is this?\n",
    "\n",
    "The difference is in the way that the measure of importance is calculated. sklearn implements feature importance by calculating the decrease in mean impurity. When training a tree, what is computed is how much each feature decreases the weighted impurity in a tree. For a forest, the impurity decrease from each feature can be averaged and the features are ranked according to this measure. However, there are some problems with this method. For one, the model can be biased in selected features with more categories. Secondly, correlated features' importance scores may be incorrect; for two or more correlated features, from the point of view from the model, any of  correlated features can be used as a predictor, with no significant 'preference' between them. However, once one of the correlated features is used, the importance of the other features are reduced, since the level of impurity that the latter correlated features can reduce has, in fact, already been reduced by that first correated feature, resulting in a lower importance score for the latter correlated features.\n",
    "\n",
    "A more stable way to look at feature importance is to randomly permute the values of the feature and measure the decrease in accuracy between models without  permuted feautre and models with the permuted feature. In the eli5 PermutationImportance class, feature importances are calculated in a similar fashion, which is what the results in the output above show. For each feature, the values are permutated, and the degree in decrease in accuracy with its values permutated is what determines the feature importance score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can check local examples to see how the values of these features affect the model's predictions. We use the INSERT HERE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"the model predicted:\")\n",
    "eli5.show_prediction(rf, doc= X_val.values[2019], \n",
    "                     show_feature_values = True, \n",
    "                     feature_names = list(X_train.columns),\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up random search\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#define number of trees \n",
    "n_estimators = [int(x) for x in np.linspace(start = 20, stop = 200, num = 20)]\n",
    "\n",
    "#define number of features to use at every split\n",
    "#max_features = ['auto', .60]\n",
    "\n",
    "#define max depth in trees\n",
    "#max_depth = [int(t) for t in np.linspace(start = 10, stop = 100, num = 10)]\n",
    "\n",
    "#define min number of samples to split a node\n",
    "#min_samples_split = [2, 5, 8]\n",
    "\n",
    "#define min number of samples at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "#define method of sample selection for each tree\n",
    "#bootstrap = [True, False]\n",
    "\n",
    "\n",
    "#create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap\n",
    "                }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#perform the random seach, using 3-fold cross_validation\n",
    "#we'll search over 100 different combinations\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "rf_random = RandomizedSearchCV(estimator=clf, param_distributions=random_grid,\n",
    "                              n_iter = 100, cv = 3,\n",
    "                               random_state = 2019, n_jobs = -1\n",
    "                              )\n",
    "\n",
    "#fit the model\n",
    "rf_random.fit(X, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the UCI dataset website:\n",
    "\n",
    "This research employed a binary variable, default payment (Yes = 1, No = 0), as the response variable. This study reviewed the literature and used the following 23 variables as explanatory variables: \n",
    "- X1: Amount of the given credit (NT dollar): it includes both the individual consumer credit and his/her family (supplementary) credit. \n",
    "- X2: Gender (1 = male; 2 = female). \n",
    "- X3: Education (1 = graduate school; 2 = university; 3 = high school; 4 = others). \n",
    "- X4: Marital status (1 = married; 2 = single; 3 = others). \n",
    "- X5: Age (year). \n",
    "- X6 - X11: History of past payment. We tracked the past monthly payment records (from April to September, 2005) as follows: X6 = the repayment status in September, 2005;\n",
    "- X7 = the repayment status in August, 2005; . . .;X11 = the repayment status in April, 2005. The measurement scale for the repayment status is: -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months; 9 = payment delay for nine months and above. \n",
    "- X12-X17: Amount of bill statement (NT dollar). X12 = amount of bill statement in September, 2005; X13 = amount of bill statement in August, 2005; . . .; X17 = amount of bill statement in April, 2005. \n",
    "- X18-X23: Amount of previous payment (NT dollar). X18 = amount paid in September, 2005; X19 = amount paid in August, 2005; . . .;X23 = amount paid in April, 2005. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
